<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="AutoMate provides the first simulation-based framework for learning specialist and generalist policies over a wide range of assemblies, as well as the first system demonstrating zero-shot sim-to-real transfer over such a range.">
  <meta name="keywords" content="Contact-rich manipulation; Robotic assembly; Sim-to-real transfer.">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>AutoMate: Specialist and Generalist Assembly Policies over Diverse Geometries</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <!-- <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>
   -->

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/robotic-arm.png">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>

  <script type="text/x-mathjax-config"> MathJax.Hub.Config({ TeX: { equationNumbers: { autoNumber: "all" } } }); </script>
   <script type="text/x-mathjax-config">
     MathJax.Hub.Config({
       tex2jax: {
         inlineMath: [ ['$','$'], ["\\(","\\)"] ],
         processEscapes: true
       }
     });
   </script>
   <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
</head>
<body>

<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">AutoMate: Specialist and Generalist Assembly Policies over Diverse Geometries</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://bingjietang718.github.io">Bingjie Tang</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="https://research.nvidia.com/person/iretiayo-akinola">Iretiayo Akinola</a><sup>2</sup>,</span>
            <span class="author-block">
              <a href="https://research.nvidia.com/person/jie-xu">Jie Xu</a><sup>2</sup>,</span>
            <span class="author-block">
              <a href="https://wenbowen123.github.io">Bowen Wen</a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a href="https://ankurhanda.github.io">Ankur Handa</a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a href="https://research.nvidia.com/person/karl-van-wyk">Karl Van Wyk</a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a href="https://research.nvidia.com/person/dieter-fox">Dieter Fox</a><sup>2,3</sup>,
            </span>
            <span class="author-block">
              <a href="https://viterbi.usc.edu/directory/faculty/Sukhatme/Gaurav">Gaurav S. Sukhatme</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="https://research.nvidia.com/person/fabio-ramos">Fabio Ramos</a><sup>2,4</sup>,
            </span>
            <span class="author-block">
              <a href="https://research.nvidia.com/person/yashraj-narang">Yashraj Narang</a><sup>2</sup>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>University of Southern California,</span>
            <span class="author-block"><sup>2</sup>NVIDIA Corporation,</span>
            <span class="author-block"><sup>3</sup>University of Washington,</span>
            <span class="author-block"><sup>4</sup>University of Sydney</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://bingjietang718.github.io/pdfs/rss2024.pdf"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://arxiv.org/abs/2407.08028"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Video Link. -->
              <span class="link-block">
                <a href="https://vimeo.com/911439543/8fa5f8134b?share=copy"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span>
              <!-- Code Link. -->
              <!-- <span class="link-block">
                <a href="https://bingjietang718.github.io/automate/"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span> -->
              <!-- Blog Link. -->
              <span class="link-block">
                <a href="https://developer.nvidia.com/blog/training-sim-to-real-transferable-robotic-assembly-skills-over-diverse-geometries/?linkId=100000272950370"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fa fa-window-restore"></i>
                  </span>
                  <span>Blog</span>
                  </a>
              </span>
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <!-- <div class="columns is-centered">
        <div class="column">
          <video id="teaser" autoplay muted loop playsinline width="100%">
            <source src="./static/videos/teaser_tactile_insertion_in_the_dark.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="column">
          <video id="teaser" autoplay muted loop playsinline width="100%">
            <source src="./static/videos/teaser_peg_placement_policy_2x.mp4"
                    type="video/mp4">
          </video>
        </div>
      </div> -->
      <img src="static/images/sim_20.gif" width="100%">
      <img src="static/images/real_20.gif" width="100%">
      <h2 class="subtitle has-text-centered">
        Deployment of AutoMate policies in simulation and reality (unique assembly IDs on top).
      </h2>
    </div>
  </div>
</section>


<!-- <embed src="static/images/teaser.pdf" style="width:100%;" frameborder="0"></embed> -->

<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Robotic assembly for high-mixture settings requires adaptivity to diverse parts and poses, which is an open challenge. Meanwhile, in other areas of robotics, large models and sim-to-real have led to tremendous progress. 
          </p>
          <p>
            Inspired by such work, we present <b>AutoMate</b>, a learning framework and system that consists of 4 parts: 1) a dataset of 100 assemblies compatible with simulation and the real world, along with parallelized simulation environments for policy learning, 2) a novel simulation-based approach for learning specialist (i.e., part-specific) policies and generalist (i.e., unified) assembly policies, 3) demonstrations of specialist policies that individually solve 80 assemblies with â‰ˆ80%+ success rates in simulation, as well as a generalist policy that jointly solves 20 assemblies with an 80%+ success rate, and 4) zero-shot sim-to-real transfer that achieves similar (or better) performance than simulation, including on perception-initialized assembly. 
          </p>
          <p>
            To our knowledge, <b>AutoMate</b> provides the first simulation-based framework for learning specialist and generalist policies over a wide range of assemblies, as well as the first system demonstrating zero-shot sim-to-real transfer over such a range.
          </p>
        </div>
        <img src="static/images/teaser.png" width="100%">
        
      </div>
    </div>
    <!--/ Abstract. -->

    <!-- Paper video. -->
    <div class="columns is-centered has-text-centered">
      <div class="column">

        
        <div class="publication-video">
          <iframe src="https://player.vimeo.com/video/962881352?h=322d43e233&amp;badge=0&amp;autopause=0&amp;player_id=0&amp;app_id=58479" frameborder="0" allow="autoplay; fullscreen; picture-in-picture; clipboard-write" ></iframe>
        </div>
        <h2 class="title is-5">Video (5 min, narrated)</h2>
      </div>
      <div class="column">
        <div class="publication-video">
          <!-- <iframe src="https://player.vimeo.com/video/911439543?h=8fa5f8134b"
                  frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe> -->
          <iframe src="https://player.vimeo.com/video/951371624?h=c1c7712dae&amp;badge=0&amp;autopause=0&amp;player_id=0&amp;app_id=58479" frameborder="0" allow="autoplay; fullscreen; picture-in-picture; clipboard-write" style="position:absolute;top:0;left:0;width:100%;height:100%;" title="AutoMate (Narrated)"></iframe>
        </div>
        <h2 class="title is-5">Video (17.4 min, narrated)</h2>
      </div>
    </div>
    <!--/ Paper video. -->
  </div>
</section>

<section class="section" id="Dataset">
  <div class="container is-max-desktop content">
    <h2 class="title">Dataset</h2>
    <div class="content has-text-justified">
      <!-- <h4 class="title">Assembly Visualization</h4> -->
      <p>
        We provide a dataset of 100 assemblies compatible with simulation and 3D printable in the real world, as well as parallelized simulation environments for all 100 assemblies. For each assembly in our dataset, we show its unique ID and a rendering. 
      </p>
      <img src="static/images/asset_lookup_table.png" width="100%">
    </div>
    <!-- <div class="content has-text-justified">
      <h4 class="title">Selected Assemblies for Real-world Deployment</h4>
      <p>
        We train a PointNet-based 3D auto-encoder to learn a latent representation of assembly geometry and use t-SNE to reduce the dimensionality of learned representations for each assembly. Here we plot the lower-dimensional representation of all 100 assemblies and visualize the 20 assemblies we randomly selected for real-world deployment.
      </p>
      <img src="static/images/selected_assembly_vis.png" width="100%">
      <iframe src="./static/videos/selected_assembly_vis.m4v"
                  width="100%" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe> 
     </div> --> 
  </div>
</section>

<section class="section" id="Specialist">
  <div class="container is-max-desktop content">
    <h2 class="title">Training Specialist (Part-specific) Policies</h2>
    <div class="content has-text-justified">
      <p>
        To train specialist policies, we propose a novel approach that combines 3 distinct algorithms:
        <ul>
          <li>assembly-by-disassembly,</li>
          <li>reinforcement learning with an imitation objective,</li>
          <li>and dynamic time warping.</li>
        </ul>  
      </p>

      <h4 class="title">Assembly-by-Disassembly</h4>
      <p>As the kinematics of assembly are a narrow passage problem, collecting human demonstrations and using motion planners are both difficult. Therefore, we collect demonstration for disassembly instead and reverse the disassembly paths for assembly:
        <ul>
          <li><b>(A)</b> command the robot to grasp the plug using a low-level controller, </li>
          <li><b>(B)</b> lift the plug from the socket and move to a randomized pose, </li>
          <li><b>(C)</b> repeat the process for additional poses, </li>
          <li><b>(D)</b> collect 100 successful disassembly paths. </li>
        </ul>  
      </p>
      <center><img src="static/images/disassembly_traj_generation.png" width="80%"></center><br>

      <h4 class="title">Reinforcement Learning with an Imitation Objective</h4>
      <p>
        We formulate the robotic assembly problem as a Markov decision process and use proximal policy optimization to learn the policy and an approximation of the value function. We found the RL-only approach proposed in our previous work <a href="https://sites.google.com/nvidia.com/industreal">IndustReal</a> is ineffective. Inspired by <a href="https://xbpeng.github.io/projects/DeepMimic/index.html">DeepMimic</a>, for locomotion, we augment RL with demonstrations by directly using an imitation reward term $R_t^I$. Our baseline reward $R_t^B$ is based on our prior work <a href="https://sites.google.com/nvidia.com/industreal">IndustReal</a>. 
      </p>
      <center>$R_t = \omega^B R_t^B + \omega^I R_t^I$</center><br>
      <p>
        We define the per-timestep imitation reward $R_t^I$ as the maximum reward over all demonstrations for the given assembly. 
      </p>
      <center>$R_t^I = \max_{i=1,...,M} R^{I_i}_t$</center><br>
      <p>
        where $M=100$ is the number of demonstrations. Conceptually, we match the current robot end-effector path with the closest demonstration path and compute imitation reward based on the distance between these two paths. 
      </p>

      <h4 class="title"><a href="https://ieeexplore.ieee.org/document/1163055">Dynamic Time Warping (DTW)</a> for Trajectory Matching</h4>
      <p>
        In our simulator, for a given path, the arc length between consecutive points is a function of the instantaneous velocity, resulting in disparate discretizations between robot end-effector paths and the collected demonstration paths. So we need to find a distance metric between paths that is insensitive to speed or sampling rate. 
      </p>
      <p>
        At each timestep, given an end-effector path, DTW uses dynamic programming to find a mapping between the end-effector path and each demonstration path that minimizes the sum of Euclidean distances between matching points. Here we show a visualization of the mapping between the end-effector path and a demonstration path by DTW.
      </p>
      <img src="static/images/dtw.png" width="100%"><br>
      <p>
        Finally, we define $R_t^{I_i} = 1-\tanh(D^{min})$.
      </p>
    </div>
  </div>
</section>

<section class="section" id="Generalist">
  <div class="container is-max-desktop content">
    <h2 class="title">Training Generalist (Unified) Policies</h2>
    <div class="content has-text-justified">
      <p>
        When training a generalist, we aim to reuse knowledge from already-trained specialists with a simple 2-stage policy distillation procedure:
        <ul>
          <li>First, we use standard behavior cloning on the specialists.</li>
          <li>Then, we use <a href="https://proceedings.mlr.press/v15/ross11a.html">DAgger</a> to reduce covariate shifts by executing the generalist policy and querying the specialists under the induced state distributions. </li>
        </ul>
        Finally, we perform a RL fine-tuning phase on the generalist policy. During the fine-tuning phase, we apply the sampling-based curriculum from our previous work <a href="https://sites.google.com/nvidia.com/industreal">IndustReal</a>.
      </p>
      <img src="static/images/generalist_methods.png" width="100%"><br>
    </div>
  </div>
</section>

<section class="section" id="Sim-to-real">
  <div class="container is-max-desktop content">
    <h2 class="title">Sim-to-Real Transfer</h2>
    <!-- <h4 class="title">Experiment Setup</h4>
    <p>
      Our experiment setup in both simulation (left) and reality (right) is shown below. 
    </p>
    <div class="columns is-centered">
      <div class="column">
        <img src="static/images/sim_robot_setup.png" width="100%">
      </div>
      <div class="column">
        <img src="static/images/real_robot_setup.png" width="100%">
      </div>
    </div> -->
    <div class="content has-text-justified">
      <p>
        We deploy our specialist policies over 20 assemblies and 200 trials and our generalist policy on the same set of 20 assemblies under the same experimental conditions. In these <b>policy-only</b> evaluations, we place the robot in lead-through, manually grasp a plug, and guide it into the socket. We then programmatically lift the plug until free from contact; apply perturbations and observation noise; and deploy a policy. 
      </p>
      <p>
        We show the per-assembly success rate in the real world compared to the simulation analogue below. The specialist and generalist policy evaluation results have shown that our simulated training conditions are sufficiently adverse to train robust and performant policies that can be transferred to the real world.
      </p>
      <img src="static/images/sim_real_eval.png" width="100%"><br><br>
      <p>
        We also evaluate our specialist and generalist policies as part of a <b>perception-initialized assembly workflow</b>. For 5 distinct assemblies, we deploy the corresponding specialist and one generalist policy, for a total of 100 trials. The specialist mean success rate is 90.0% and the generalist mean success rate is 86%. These results indicate that 6-DOF pose estimation, grasp optimization, and our proposed methods for learning specialist and generalist policies can be effectively combined to achieve reliable assembly under realistic conditions using research-grade hardware.
      </p>
      <img src="static/images/perception_initialized_workflow.png" width="100%"><br><br>
      <!-- <h4 class="title">Perception</h4> -->
      
    </div>
  </div>
</section>

<section class="section" id="Perception">
  <div class="container is-max-desktop content">
    <h2 class="title">Additional Detail: Perception Pipeline</h2>
    <p>
        Our perception pipeline in reality is based on <a href="https://github.com/facebookresearch/segment-anything">Segment Anything (SAM)</a> and <a href="https://nvlabs.github.io/FoundationPose/">FoundationPose</a>. To estimate the plug or socket pose, the wrist-mounted <a href="https://www.intelrealsense.com/depth-camera-d435/">Intel RealSense D435 RGB-D camera</a> first captures an RGB image and a depth image. The RGB image is shown to the user, who clicks on the plug or socket of interest. The RGB image and pixel coordinates from user clicks are then passed through SAM and SAM outputs a segmentation mask for the plug or socket. The RGB image, depth image, segmentation mask and CAD model for the plug or socket are passed through FoundationPose to estimate the 6-DoF pose in the camera frame. Finally, we convert the pose to the robot frame using robot kinematics and camera extrinsics.
      </p>
      <img src="static/images/perception_pipeline.png" width="100%">
  </div>
</section>


<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@inproceedings{tang2024automate,
  author    = {Tang, Bingjie and Akinola, Iretiayo and Xu, Jie and Wen, Bowen and Handa, Ankur and Van Wyk, Karl and Fox, Dieter and S. Sukhatme, Gaurav and Ramos, Fabio and Narang, Yashraj},
  title     = {AutoMate: Specialist and Generalist Assembly Policies over Diverse Geometries},
  booktitle = {Robotics: Science and Systems},
  year      = {2024},
}</code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            Website template borrowed from
              <a href="https://github.com/nerfies/nerfies.github.io">Nerfies</a>.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>